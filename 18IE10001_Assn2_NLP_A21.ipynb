{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# **Assignment-2 for CS60075: Natural Language Processing**\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
    "\n",
    "#### Date of Announcement: 15th Sept, 2021\n",
    "#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n",
    "#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset. This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit with outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ElRkQElWUMjG"
   },
   "outputs": [],
   "source": [
    "import re, nltk, keras, string, html, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fhHRim2AUm4z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the IMDB dataset. We load it using pandas as dataframe\n",
    "data = pd.read_csv('https://drive.google.com/uc?id=1EVCXDTYdQHVKa4NbA0Mf1dyj46Mp5tLV')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_Hn2f6VMP7"
   },
   "source": [
    "# Preprocessing\n",
    "Pre-precessing that needs to be done on lower cased corpus\n",
    "\n",
    "1. Removal of html tags\n",
    "2. Removal of  URLS\n",
    "3. Removal of non alphanumeric character\n",
    "4. Removal of Stopwords\n",
    "5. Performing stemming and lemmatization\n",
    "\n",
    "We use regex from re. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/abj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/abj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "      <td>[one, reviewer, mentioned, watching, 1, oz, ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>[basically, family, little, boy, jake, think, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>[petter, mattei, love, time, money, visually, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought movie right good job creative original...</td>\n",
       "      <td>[thought, movie, right, good, job, creative, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>[bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>catholic taught parochial elementary school nu...</td>\n",
       "      <td>[catholic, taught, parochial, elementary, scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>going disagree previous comment side maltin on...</td>\n",
       "      <td>[going, disagree, previous, comment, side, mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one expects star trek movie high art fan expec...</td>\n",
       "      <td>[one, expects, star, trek, movie, high, art, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      one of the other reviewers has mentioned that ...  positive   \n",
       "1      a wonderful little production. <br /><br />the...  positive   \n",
       "2      i thought this was a wonderful way to spend ti...  positive   \n",
       "3      basically there's a family where a little boy ...  negative   \n",
       "4      petter mattei's \"love in the time of money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  i thought this movie did a down right good job...  positive   \n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  i am a catholic taught in parochial elementary...  negative   \n",
       "49998  i'm going to have to disagree with the previou...  negative   \n",
       "49999  no one expects the star trek movies to be high...  negative   \n",
       "\n",
       "                                                 cleaned  \\\n",
       "0      one reviewer mentioned watching 1 oz episode h...   \n",
       "1      wonderful little production filming technique ...   \n",
       "2      thought wonderful way spend time hot summer we...   \n",
       "3      basically family little boy jake think zombie ...   \n",
       "4      petter mattei love time money visually stunnin...   \n",
       "...                                                  ...   \n",
       "49995  thought movie right good job creative original...   \n",
       "49996  bad plot bad dialogue bad acting idiotic direc...   \n",
       "49997  catholic taught parochial elementary school nu...   \n",
       "49998  going disagree previous comment side maltin on...   \n",
       "49999  one expects star trek movie high art fan expec...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [one, reviewer, mentioned, watching, 1, oz, ep...  \n",
       "1      [wonderful, little, production, filming, techn...  \n",
       "2      [thought, wonderful, way, spend, time, hot, su...  \n",
       "3      [basically, family, little, boy, jake, think, ...  \n",
       "4      [petter, mattei, love, time, money, visually, ...  \n",
       "...                                                  ...  \n",
       "49995  [thought, movie, right, good, job, creative, o...  \n",
       "49996  [bad, plot, bad, dialogue, bad, acting, idioti...  \n",
       "49997  [catholic, taught, parochial, elementary, scho...  \n",
       "49998  [going, disagree, previous, comment, side, mal...  \n",
       "49999  [one, expects, star, trek, movie, high, art, f...  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "data[\"review\"]= data[\"review\"].str.lower() \n",
    "data[\"sentiment\"]= data[\"sentiment\"].str.lower()             #converts every value in the column to lowercase\n",
    "\n",
    "def cleaning(data):\n",
    "    clean = re.sub('<.*?>', ' ', str(data))            #removes HTML tags\n",
    "    clean = re.sub('\\'.*?\\s',' ', clean)               #removes all hanging letters afer apostrophes (s in it's)\n",
    "    clean = re.sub(r'http\\S+',' ', clean)              #removes URLs\n",
    "    clean = re.sub('\\W+',' ', clean)                   #replacing the non alphanumeric characters\n",
    "    return html.unescape(clean)\n",
    "data['cleaned'] = data['review'].apply(cleaning)\n",
    "\n",
    "\n",
    "def tokenizing(data):\n",
    "    review = data['cleaned']                            #tokenizing is done\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    return tokens\n",
    "data['tokens'] = data.apply(tokenizing, axis=1)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stops(data):\n",
    "    my_list = data['tokens']\n",
    "    meaningful_words = [w for w in my_list if not w in stop_words]           #stopwords are removed from the tokenized data\n",
    "    return (meaningful_words)\n",
    "data['tokens'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(data):\n",
    "    my_list = data['tokens']\n",
    "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_list]    #lemmatizing is performed. It's more efficient and better than stemming.\n",
    "    return (lemmatized_list)\n",
    "data['tokens'] = data.apply(lemmatizing, axis=1)\n",
    "\n",
    "def rejoin_words(data):\n",
    "    my_list = data['tokens']\n",
    "    joined_words = ( \" \".join(my_list))                     #rejoins all stemmed words\n",
    "    return joined_words\n",
    "data['cleaned'] = data.apply(rejoin_words, axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DyaSkfcvYGXk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is -  544935\n",
      "\n",
      "The number of tokens is -  5961690\n",
      "\n",
      "The average number of tokens per sentence is -  11\n",
      "\n",
      "The number of positive examples are -  25000\n",
      "\n",
      "The number of negative examples are -  25000\n",
      "\n",
      "The proportion of positive sentiments to total ones are -  0.5\n",
      "\n",
      "The proportion of negative sentiments to total ones are -  0.5\n",
      "\n",
      "The proportion of positive sentiments to negative ones are -  1.0\n"
     ]
    }
   ],
   "source": [
    "# Print statistics of Data like avg length of sentence , proportion of data w.r.t class labels\n",
    "def sents(data):\n",
    "    clean = re.sub('<.*?>', ' ', str(data))            #removes HTML tags\n",
    "    clean = re.sub('\\'.*?\\s',' ', clean)               #removes all hanging letters afer apostrophes (s in it's)\n",
    "    clean = re.sub(r'http\\S+',' ', clean)              #removes URLs\n",
    "    clean = re.sub('[^a-zA-Z0-9\\.]+',' ', clean)       #removes all non-alphanumeric characters except periods.\n",
    "    tokens = nltk.sent_tokenize(clean)                 #sentence tokenizing is done\n",
    "    return tokens\n",
    "sents = data['review'].apply(sents)\n",
    "\n",
    "length_s = 0\n",
    "for i in range(data.shape[0]):\n",
    "    length_s+= len(sents[i])\n",
    "print(\"The number of sentences is - \", length_s)          #prints the number of sentences\n",
    "\n",
    "length_t = 0\n",
    "for i in range(data.shape[0]):\n",
    "    length_t+= len(data['tokens'][i])\n",
    "print(\"\\nThe number of tokens is - \", length_t)           #prints the number of tokens\n",
    "\n",
    "average_tokens = round(length_t/length_s)\n",
    "print(\"\\nThe average number of tokens per sentence is - \", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "positive = negative = 0\n",
    "for i in range(data.shape[0]):\n",
    "    if (data['sentiment'][i]=='positive'):\n",
    "        positive += 1                           #finds the proprtion of positive and negative sentiments\n",
    "    else:\n",
    "        negative += 1\n",
    "\n",
    "print(\"\\nThe number of positive examples are - \", positive)\n",
    "print(\"\\nThe number of negative examples are - \", negative)\n",
    "print(\"\\nThe proportion of positive sentiments to total ones are - \", positive/(positive + negative))\n",
    "print(\"\\nThe proportion of negative sentiments to total ones are - \", negative/(positive + negative))\n",
    "print(\"\\nThe proportion of positive sentiments to negative ones are - \", positive/negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkJ-e2pUwun"
   },
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eVq-mN28U_J4"
   },
   "outputs": [],
   "source": [
    "# get reviews column from df\n",
    "reviews = data['cleaned'].values\n",
    "\n",
    "# get labels column from df\n",
    "labels = data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ljo5NquhXTXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: encoded, dtype: int64\n",
      "\n",
      "The encoded classes are -  {'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "# Use label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "data['encoded']= encoded_labels\n",
    "print(data['encoded'].head())\n",
    "\n",
    "# print(enc.classes_)\n",
    "encoder_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"\\nThe encoded classes are - \", encoder_mapping)\n",
    "\n",
    "labels = data['encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wzG-C_EVWWET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training sentences are -\n",
      "\n",
      "['caught little gem totally accident back 1980 revival theatre see two old silly sci fi movie theatre packed full warning showed bunch sci fi short spoof get u mood somewhat amusing came within second audience hysteric biggest laugh came showed princess laia huge cinnamon bun instead hair head look camera give grim smile nod made even funnier got ta see chewabacca played look like muppet extremely silly stupid stop laughing dialogue drowned laughter also know star war pretty well even funnier deliberately poke fun dialogue really work audience definite 10'\n",
      " 'believe let movie accomplish favor friend ask early april 14 2007 movie certainly pain as theater sickly boring even felt gory impact daunting scene deem complete failure attract audience worst even trampled cause friend failed come time theater busy assisting boyfriend looking appropriate lodge stay one night really disappointed matter movie matter indeed poor plot useless storyline naively created know say anymore title suggest anyway creep horror failed overture u viewer maybe beating animal could get creep show theater real situational play good luck anyone attempt watch anyway'\n",
      " 'spoiler alert get nerve people remake use term loosely good movie american version dutch thriller someone decided original ending pasteurized enough american audience create new one stupid improbable pretend kind ending favor get original one'\n",
      " ...\n",
      " 'waste time danger watch tempted tear dvd wall heave thru window amateur production terrible repetitive vacuous dialog paper thin plot line wooden performance lucy lawless pathetically hackneyed seriously flawed story completely unbelievable character two worst concept film v 1 evil twin 2 amnesia twin plot twist outrageously simplistic obvious like watching train coming track middle day prairie even resolve properly evil punished original crime please please please watch even free choice go synagogue'\n",
      " 'far pathetic movie indian cinema cinema come totally piece crap story line ode hold water shameful respectful actor stooped low making disgraceful movie little respect lost forever retired longtime ago instead making fool loosing self respect would recommend movie one furthermore would suggest amitabh retire already wonder imdb given recommendation movie movie horrible recommendation could made seeing movie devdas line movie grave injustice decent movie overall could say shameless'\n",
      " 'movie forever left impression watched freshman high school home alone night think lost respect robert reed actor huge fan brady bunch also thought role chuck connor horrendous evil however movie made impact volunteer woman state prison bible study church service trying change woman life one time fascinates people actually watched movie none friend watched family clueless day discus movie see']\n",
      "\n",
      "The test sentences are -\n",
      "\n",
      "['yes mtv really way market daria started clever teenage angst comment everything suck make viewer feel better sucky teenage life sitcom mutated deal problem charade used watch daria time loved sitting watching called movie wonder point daria tell u lead life college excuse point daria made every episode like ok long ok matter rest sick sad world think entire thing reminded scene reality bite movie channel show documentry first time'\n",
      " 'story bride fair amusing engaging one filmmaker credit set portray rural minnesotan respect ordinarily reserved coast dweller weird though find independent movie brainchild single person unambitious clichÃ© ridden committee brewed hollywood potboiler portrait rural people intended affectionate think character ring true quite meal small town diner never overheard debate merit different nineteenth century english novelist one might suggest writer director semans experience rural culture coen brother considerably le satiric verve'\n",
      " 'team varied scully mulder two scientist pilot guy play bana seinfeld go arctic research post member died either killing killing discover worm virus parasitic point madness death problem certain dog lash anyone could infected favorite episode season 1 also one favorite show arctic environment encloses character course like carpenter thing lot fun watching even tempered character suddenly start flip dramatic scene visual effect worm effect skin cheesy mind drama character end working would usually tension actor including bana guy understand going story usual loved ambiguity ending highly recommended'\n",
      " ...\n",
      " 'film seen made uwe boll knew probably worst director ever always make film based video game also house dead one imdb bottom 100 still wanted watch huge fan game wanted see doe film make bad watching agree crap movie story first 15 20 minute nothing topless teenage girl brain running moment wondering zombie brain dead girl night time zombie popped nowhere started attacking people later woman started shooting mean take one place every 5 minute supposed comedy horror knew fell asleep second half woke end credit manage watch good thing film true insult classic game uwe boll please make film thank'\n",
      " 'story shin ae move milyang seoul young son jun start accidental death husband husband born opening piano school also ambition land insurance money received death film probably would like hollywood film falling local guy happy son new home hollywood son get kidnapped murdered ostensibly known cash settlement grief process attempt moving attempt clear conscience guilt done admirably lead actress superb caveat stated depressing film know going want shin ae go grief find measure happiness hollywood korea korean cinema especially drama pull punch life happens great acting sometimes tough film watch due going stay rewarded'\n",
      " 'leon errol handle double role uncle matt lindsay lord basil epping superbly trouble liking mexican spitfire series contrived produce mistaken identity telegraphed way advance errol funny stuffy lord epping would preferred lot wit much le repetition']\n",
      "\n",
      "The training labels are -\n",
      "\n",
      "47808    1\n",
      "20154    0\n",
      "43069    0\n",
      "19413    0\n",
      "13673    0\n",
      "        ..\n",
      "31092    1\n",
      "22917    0\n",
      "47481    0\n",
      "35597    0\n",
      "27491    0\n",
      "Name: encoded, Length: 40000, dtype: int64\n",
      "\n",
      "The test labels are -\n",
      "\n",
      "18870    0\n",
      "39791    0\n",
      "30381    1\n",
      "42294    0\n",
      "33480    0\n",
      "        ..\n",
      "3634     0\n",
      "47910    0\n",
      "16086    0\n",
      "48294    1\n",
      "4478     0\n",
      "Name: encoded, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test (80% - 20%). \n",
    "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# train_sentences, test_sentences, train_labels, test_labels\n",
    "print(\"The training sentences are -\",train_sentences, sep='\\n\\n')\n",
    "print(\"\\nThe test sentences are -\",test_sentences, sep='\\n\\n')\n",
    "print(\"\\nThe training labels are -\",train_labels, sep='\\n\\n')\n",
    "print(\"\\nThe test labels are -\",test_labels, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz1YdsSkiWCX"
   },
   "source": [
    "There are two approaches possible for building vocabulary for the Naive Bayes classifier.\n",
    "1. We take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
    "2. We take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one of the probability terms are not zero.\n",
    " \n",
    "We use the 2nd approach.\n",
    " \n",
    "Also, building vocab by taking all words in the train set is memory intensive, hence we build the vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
    "\n",
    "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
    "\n",
    "\n",
    "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
    "\n",
    "$N_{w_j}$ : Total count of features in class $w_j$\n",
    "\n",
    "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
    "\n",
    "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1cllNfGmUr77"
   },
   "outputs": [],
   "source": [
    "# Use Count vectorizer to get frequency of the words\n",
    "vectorizer = CountVectorizer(max_features = 3000)\n",
    "\n",
    "sents_encoded = vectorizer.fit_transform(train_sentences)         #encodes all training sentences\n",
    "counts = sents_encoded.sum(axis=0).A1\n",
    "vocab = list(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iE7pxWIYW1z0"
   },
   "outputs": [],
   "source": [
    "# Use laplace smoothing for words in test set not present in vocab of train set\n",
    "class MultinomialNaiveBayes:\n",
    "  \n",
    "    def __init__(self, classes, tokenizer):\n",
    "      #self.tokenizer = tokenizer\n",
    "      self.classes = classes\n",
    "      \n",
    "    def group_by_class(self, X, y):\n",
    "      data = dict()\n",
    "      for c in self.classes:                            #grouping by positive and negative sentiments\n",
    "        data[c] = X[np.where(y == c)]\n",
    "      return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = vocab                            #using the pre-made vocabulary of 3000 most frequent training words\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "          self.n_class_items[c] = len(data)\n",
    "          self.log_class_priors[c] = math.log(self.n_class_items[c] / n)   #taking log for easier calculation\n",
    "          self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "          for text in data:\n",
    "            counts = Counter(nltk.word_tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                self.word_counts[c][word] += count\n",
    "                \n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class):          #smoothing\n",
    "      num = self.word_counts[text_class][word] + 1\n",
    "      denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "      return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:\n",
    "          \n",
    "          class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "          words = set(nltk.word_tokenize(text))\n",
    "          for word in words:\n",
    "              if word not in self.vocab: continue\n",
    "\n",
    "              for c in self.classes:\n",
    "                \n",
    "                log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                class_scores[c] += log_w_given_c\n",
    "                \n",
    "          result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AtQSl1zvW4DD"
   },
   "outputs": [],
   "source": [
    "# Build the model.Don't use the model from sklearn\n",
    "MNB = MultinomialNaiveBayes(\n",
    "    classes=np.unique(labels), \n",
    "    tokenizer=Tokenizer()\n",
    ").fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the MNB classifier is  0.8533\n",
      "\n",
      "The classification report with metrics - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      5000\n",
      "           1       0.85      0.86      0.85      5000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model on test set and report Accuracy\n",
    "predicted_labels = MNB.predict(test_sentences)\n",
    "print(\"The accuracy of the MNB classifier is \", accuracy_score(test_labels, predicted_labels))\n",
    "print(\"\\nThe classification report with metrics - \\n\", classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNql0acU7sa"
   },
   "source": [
    "# LSTM based Classifier\n",
    "\n",
    "We use the above train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 150\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SkqnvbUOXoN0"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# convert train dataset to sequence and pads sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# convert Test dataset to sequence and pads sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Mtw3w895ZP39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 100)          8295000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 8,382,601\n",
      "Trainable params: 8,382,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "skmaDJMnZTzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 202s 176ms/step - loss: 0.3602 - accuracy: 0.8418 - val_loss: 0.2779 - val_accuracy: 0.8917\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 195s 174ms/step - loss: 0.1639 - accuracy: 0.9419 - val_loss: 0.3351 - val_accuracy: 0.8755\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 196s 174ms/step - loss: 0.0848 - accuracy: 0.9719 - val_loss: 0.4259 - val_accuracy: 0.8802\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 191s 170ms/step - loss: 0.0437 - accuracy: 0.9859 - val_loss: 0.5305 - val_accuracy: 0.8798\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 194s 172ms/step - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.4879 - val_accuracy: 0.8815\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TjEhWEr5Zq7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities are - \n",
      "[[0.9931212 ]\n",
      " [0.05766219]\n",
      " [0.9997386 ]\n",
      " ...\n",
      " [0.00148791]\n",
      " [0.9930549 ]\n",
      " [0.34682545]]\n",
      "\n",
      "The labels are - \n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "The accuracy of the model is  0.882\n",
      "\n",
      "The accuracy and other metrics are \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5000\n",
      "           1       0.89      0.88      0.88      5000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on Test data\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# Get probabilities\n",
    "print(\"The probabilities are - \", prediction, sep='\\n')\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "for each in prediction:\n",
    "    if each[0] >=0.5:\n",
    "        each[0] = 1\n",
    "    else:\n",
    "        each[0] = 0\n",
    "prediction = prediction.astype('int64') \n",
    "print(\"\\nThe labels are - \", prediction, sep='\\n')\n",
    "\n",
    "# Accuracy : one can use classification_report from sklearn\n",
    "print(\"\\nThe accuracy of the model is \", accuracy_score(test_labels, prediction))\n",
    "print(\"\\nThe accuracy and other metrics are \\n\", classification_report(test_labels, prediction, labels=[0, 1]),sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIICV-ySOYL0"
   },
   "source": [
    "## Get predictions for random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "m2RmfNL3OYL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities are - \n",
      "[[0.9454621 ]\n",
      " [0.08515254]\n",
      " [0.07087687]]\n",
      "\n",
      "The labels are - \n",
      "[[1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the movie plot is terrible but it had good acting\"]\n",
    "\n",
    "# convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "# pad the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get probabilities\n",
    "prediction = model.predict(padded)\n",
    "print(\"The probabilities are - \", prediction, sep='\\n')\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "for each in prediction:\n",
    "    if each[0] >=0.5:\n",
    "        each[0] = 1\n",
    "    else:\n",
    "        each[0] = 0\n",
    "prediction = prediction.astype('int64') \n",
    "print(\"\\nThe labels are - \", prediction, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB classifier has an accuracy of 85.33%\n",
    "### LSTM classifier has an accuracy of 88.2%\n",
    "### So, LSTM is the better classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
